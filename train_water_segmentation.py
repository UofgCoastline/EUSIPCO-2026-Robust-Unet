#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ·æ°´åŒºåŸŸè¯­ä¹‰åˆ†å‰²æ¨¡å‹è®­ç»ƒç¨‹åº
åŸºäºlabelmeæ ‡æ³¨çš„æµ·æ°´åŒºåŸŸè®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹

ä½œè€…: CoastSatæµ·å²¸çº¿æå–åŠ©æ‰‹
åˆ›å»ºæ—¥æœŸ: 2025-01-26
"""

import os
import sys
import json
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib
# é…ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Liberation Sans', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from datetime import datetime
import pickle
from osgeo import gdal
import warnings
warnings.filterwarnings('ignore')

class WaterSegmentationDataset(Dataset):
    """æµ·æ°´åŒºåŸŸåˆ†å‰²æ•°æ®é›†"""
    
    def __init__(self, image_paths, label_paths, transform=None, image_size=(512, 512)):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        å‚æ•°:
        image_paths: list, å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        label_paths: list, æ ‡æ³¨æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        transform: torchvision.transforms, æ•°æ®å˜æ¢
        image_size: tuple, è¾“å…¥å›¾åƒå°ºå¯¸
        """
        self.image_paths = image_paths
        self.label_paths = label_paths
        self.transform = transform
        self.image_size = image_size
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # è¯»å–å›¾åƒï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        image_path = self.image_paths[idx]
        image = self.load_image(image_path)
        
        # è¯»å–æ ‡æ³¨å¹¶ç”Ÿæˆmask
        mask = self.create_mask_from_labelme(self.label_paths[idx], image.size)
        
        # è°ƒæ•´å°ºå¯¸
        image = image.resize(self.image_size)
        mask = Image.fromarray(mask).resize(self.image_size, Image.NEAREST)
        mask = np.array(mask)
        
        # æ•°æ®å˜æ¢
        if self.transform:
            image = self.transform(image)
        else:
            image = transforms.ToTensor()(image)
        
        mask = torch.from_numpy(mask).long()
        
        return image, mask
    
    def load_image(self, image_path):
        """
        åŠ è½½å›¾åƒ - é’ˆå¯¹convertedæ–‡ä»¶å¤¹ä¸­çš„PNGå’ŒåŸå§‹TIFè¿›è¡Œä¼˜åŒ–
        
        å‚æ•°:
        image_path: str, å›¾åƒæ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        PIL.Image: RGBå›¾åƒ
        """
        try:
            if image_path.lower().endswith(('.tif', '.tiff')):
                # å¤„ç†åŸå§‹TIFæ ¼å¼ï¼ˆä¸tif_to_image.pyä¿æŒä¸€è‡´çš„æ°´ä½“å¢å¼ºï¼‰
                return self.load_tif_with_water_enhancement(image_path)
            else:
                # å¸¸è§„å›¾åƒæ ¼å¼ï¼ŒåŒ…æ‹¬ä»TIFè½¬æ¢åçš„PNG
                # è¿™äº›PNGå·²ç»ç»è¿‡äº†æ°´ä½“å¢å¼ºå¤„ç†
                return Image.open(image_path).convert('RGB')
                
        except Exception as e:
            print(f"åŠ è½½å›¾åƒå¤±è´¥ {image_path}: {e}")
            # è¿”å›ç©ºç™½å›¾åƒ
            return Image.new('RGB', (512, 512), (0, 0, 0))
    
    def load_tif_with_water_enhancement(self, tif_path):
        """
        åŠ è½½TIFæ–‡ä»¶å¹¶åº”ç”¨æ°´ä½“å¢å¼ºï¼ˆä¸tif_to_image.pyä¿æŒä¸€è‡´ï¼‰
        
        å‚æ•°:
        tif_path: str, TIFæ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        PIL.Image: RGBå›¾åƒ
        """
        dataset = gdal.Open(tif_path)
        if dataset is None:
            raise ValueError(f"æ— æ³•æ‰“å¼€TIFæ–‡ä»¶: {tif_path}")
        
        # è¯»å–æ³¢æ®µæ•°æ®ï¼ˆä¸è½¬æ¢ç¨‹åºä¿æŒä¸€è‡´ï¼‰
        bands = []
        for i in range(1, min(dataset.RasterCount + 1, 7)):  # æœ€å¤šè¯»å–6ä¸ªæ³¢æ®µ
            band = dataset.GetRasterBand(i)
            data = band.ReadAsArray()
            bands.append(data)
        
        bands = np.array(bands)
        
        # åˆ›å»ºRGBå›¾åƒï¼ˆä¸tif_to_image.pyçš„é€»è¾‘ä¸€è‡´ï¼‰
        if bands.shape[0] >= 3:
            if bands.shape[0] >= 4:
                # ä½¿ç”¨NIR-Red-Greenç»„åˆçªå‡ºæ°´ä½“ï¼ˆä¸è½¬æ¢ç¨‹åºä¸€è‡´ï¼‰
                try:
                    rgb = np.dstack([bands[4], bands[3], bands[2]])  # NIR, Red, Green
                except IndexError:
                    rgb = np.dstack([bands[2], bands[1], bands[0]])  # æ ‡å‡†RGB
            else:
                rgb = np.dstack([bands[2], bands[1], bands[0]])  # Red, Green, Blue
        else:
            # ç°åº¦å›¾åƒ
            gray = bands[0]
            rgb = np.dstack([gray, gray, gray])
        
        # å›¾åƒå¢å¼ºï¼ˆä¸è½¬æ¢ç¨‹åºä¸€è‡´ï¼‰
        rgb_enhanced = self.enhance_image_for_water(rgb)
        return Image.fromarray(rgb_enhanced.astype(np.uint8))
    
    def enhance_image_for_water(self, rgb):
        """
        å¢å¼ºå›¾åƒå¯¹æ¯”åº¦ï¼Œçªå‡ºæ°´ä½“åŒºåŸŸï¼ˆä¸tif_to_image.pyä¿æŒä¸€è‡´ï¼‰
        
        å‚æ•°:
        rgb: numpy.ndarray, RGBå›¾åƒæ•°ç»„
        
        è¿”å›:
        numpy.ndarray: å¢å¼ºåçš„å›¾åƒ
        """
        enhanced = np.zeros_like(rgb)
        
        for i in range(rgb.shape[2]):
            band = rgb[:, :, i]
            
            # è®¡ç®—ç™¾åˆ†ä½æ•°è¿›è¡Œæ‹‰ä¼¸
            p2, p98 = np.percentile(band, [2, 98])
            
            # çº¿æ€§æ‹‰ä¼¸
            band_stretched = np.clip((band - p2) / (p98 - p2) * 255, 0, 255)
            
            # å¯¹æ°´ä½“åŒºåŸŸè¿›è¡Œé¢å¤–å¢å¼º
            if i == 0:  # å‡è®¾ç¬¬ä¸€ä¸ªæ³¢æ®µæ˜¯è¿‘çº¢å¤–æˆ–çº¢è‰²
                # å¢å¼ºä½å€¼åŒºåŸŸï¼ˆå¯èƒ½æ˜¯æ°´ä½“ï¼‰
                mask = band_stretched < 100
                band_stretched[mask] = band_stretched[mask] * 0.7  # é™ä½äº®åº¦çªå‡ºæ°´ä½“
            
            enhanced[:, :, i] = band_stretched
        
        return enhanced
    
    def create_mask_from_labelme(self, label_path, image_size):
        """
        ä»labelmeæ ‡æ³¨æ–‡ä»¶åˆ›å»ºåˆ†å‰²mask
        
        å‚æ•°:
        label_path: str, labelmeæ ‡æ³¨æ–‡ä»¶è·¯å¾„
        image_size: tuple, å›¾åƒå°ºå¯¸ (width, height)
        
        è¿”å›:
        numpy.ndarray: åˆ†å‰²mask (0-å…¶ä»–, 1-æµ·æ°´)
        """
        try:
            with open(label_path, 'r', encoding='utf-8') as f:
                label_data = json.load(f)
            
            # åˆ›å»ºç©ºç™½mask
            mask = np.zeros((image_size[1], image_size[0]), dtype=np.uint8)
            
            # éå†æ‰€æœ‰å½¢çŠ¶
            for shape in label_data.get('shapes', []):
                if shape['label'].lower() in ['water', 'sea', 'æµ·æ°´', 'æ°´ä½“']:
                    # è·å–å¤šè¾¹å½¢ç‚¹
                    points = np.array(shape['points'], dtype=np.int32)
                    
                    # å¡«å……å¤šè¾¹å½¢åŒºåŸŸ
                    cv2.fillPoly(mask, [points], 1)
            
            return mask
            
        except Exception as e:
            print(f"è¯»å–æ ‡æ³¨æ–‡ä»¶å¤±è´¥ {label_path}: {e}")
            return np.zeros((image_size[1], image_size[0]), dtype=np.uint8)

class UNet(nn.Module):
    """U-Netè¯­ä¹‰åˆ†å‰²ç½‘ç»œ"""
    
    def __init__(self, n_channels=3, n_classes=2):
        """
        åˆå§‹åŒ–U-Net
        
        å‚æ•°:
        n_channels: int, è¾“å…¥é€šé“æ•°
        n_classes: int, åˆ†ç±»ç±»åˆ«æ•°
        """
        super(UNet, self).__init__()
        
        # ç¼–ç å™¨
        self.enc1 = self.conv_block(n_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)
        
        # ç“¶é¢ˆå±‚
        self.bottleneck = self.conv_block(512, 1024)
        
        # è§£ç å™¨
        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.dec4 = self.conv_block(1024, 512)
        
        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = self.conv_block(512, 256)
        
        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = self.conv_block(256, 128)
        
        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = self.conv_block(128, 64)
        
        # è¾“å‡ºå±‚
        self.final = nn.Conv2d(64, n_classes, kernel_size=1)
        
        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
    
    def conv_block(self, in_channels, out_channels):
        """å·ç§¯å—"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # ç¼–ç è·¯å¾„
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        
        # ç“¶é¢ˆ
        bottleneck = self.bottleneck(self.pool(enc4))
        
        # è§£ç è·¯å¾„
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)
        dec4 = self.dec4(dec4)
        
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)
        
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        return self.final(dec1)

class WaterSegmentationTrainer:
    """æµ·æ°´åˆ†å‰²æ¨¡å‹è®­ç»ƒå™¨ - é›†æˆCoastSatè®­ç»ƒæ€æƒ³"""
    
    def __init__(self, device='cpu'):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        å‚æ•°:
        device: str, è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.model = UNet(n_channels=3, n_classes=2).to(device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆCoastSaté£æ ¼ï¼‰
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=10
        )
        
        # æ•°æ®å˜æ¢
        self.train_transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.RandomHorizontalFlip(0.5),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        self.val_transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # è®­ç»ƒå†å²è®°å½•ï¼ˆå‚è€ƒCoastSatçš„è®°å½•æ–¹å¼ï¼‰
        self.training_history = {
            'train_losses': [],
            'val_losses': [],
            'learning_rates': [],
            'accuracies': [],
            'iou_scores': [],
            'best_model_epoch': 0,
            'training_time': 0
        }
    
    def calculate_iou(self, pred_mask, true_mask):
        """
        è®¡ç®—IoUåˆ†æ•°ï¼ˆå‚è€ƒCoastSatçš„è¯„ä¼°æ–¹æ³•ï¼‰
        
        å‚æ•°:
        pred_mask: torch.Tensor, é¢„æµ‹mask
        true_mask: torch.Tensor, çœŸå®mask
        
        è¿”å›:
        float: IoUåˆ†æ•°
        """
        intersection = torch.logical_and(pred_mask, true_mask).sum().float()
        union = torch.logical_or(pred_mask, true_mask).sum().float()
        
        if union == 0:
            return 1.0
        
        return (intersection / union).item()
    
    def validate_model(self, val_loader):
        """
        éªŒè¯æ¨¡å‹æ€§èƒ½ï¼ˆé›†æˆCoastSatçš„éªŒè¯æ€æƒ³ï¼‰
        
        å‚æ•°:
        val_loader: DataLoader, éªŒè¯æ•°æ®åŠ è½½å™¨
        
        è¿”å›:
        dict: éªŒè¯ç»“æœ
        """
        self.model.eval()
        total_loss = 0.0
        total_iou = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for images, masks in val_loader:
                images, masks = images.to(self.device), masks.to(self.device)
                
                outputs = self.model(images)
                loss = self.criterion(outputs, masks)
                
                # è®¡ç®—é¢„æµ‹ç»“æœ
                pred_masks = torch.argmax(outputs, dim=1)
                
                # è®¡ç®—æŒ‡æ ‡
                accuracy = (pred_masks == masks).float().mean()
                iou = self.calculate_iou(pred_masks == 1, masks == 1)
                
                total_loss += loss.item()
                total_accuracy += accuracy.item()
                total_iou += iou
                num_batches += 1
        
        return {
            'loss': total_loss / num_batches,
            'accuracy': total_accuracy / num_batches,
            'iou': total_iou / num_batches
        }
    
    def create_training_visualization(self, epoch, train_loss, val_metrics, save_dir):
        """
        åˆ›å»ºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ï¼ˆå‚è€ƒCoastSatçš„å¯è§†åŒ–é£æ ¼ï¼‰
        
        å‚æ•°:
        epoch: int, å½“å‰è½®æ¬¡
        train_loss: float, è®­ç»ƒæŸå¤±
        val_metrics: dict, éªŒè¯æŒ‡æ ‡
        save_dir: str, ä¿å­˜ç›®å½•
        """
        # ç¡®ä¿ä¸­æ–‡å­—ä½“æ­£ç¡®æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Liberation Sans', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.training_history['train_losses'], 'b-', label='è®­ç»ƒæŸå¤±')
        axes[0, 0].plot(self.training_history['val_losses'], 'r-', label='éªŒè¯æŸå¤±')
        axes[0, 0].set_title('è®­ç»ƒæŸå¤±å˜åŒ–')
        axes[0, 0].set_xlabel('è½®æ¬¡')
        axes[0, 0].set_ylabel('æŸå¤±å€¼')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(self.training_history['accuracies'], 'g-', label='éªŒè¯å‡†ç¡®ç‡')
        axes[0, 1].set_title('æ¨¡å‹å‡†ç¡®ç‡å˜åŒ–')
        axes[0, 1].set_xlabel('è½®æ¬¡')
        axes[0, 1].set_ylabel('å‡†ç¡®ç‡')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # IoUåˆ†æ•°æ›²çº¿
        axes[1, 0].plot(self.training_history['iou_scores'], 'm-', label='IoUåˆ†æ•°')
        axes[1, 0].set_title('IoUåˆ†æ•°å˜åŒ–')
        axes[1, 0].set_xlabel('è½®æ¬¡')
        axes[1, 0].set_ylabel('IoUåˆ†æ•°')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡å˜åŒ–
        axes[1, 1].plot(self.training_history['learning_rates'], 'orange', label='å­¦ä¹ ç‡')
        axes[1, 1].set_title('å­¦ä¹ ç‡å˜åŒ–')
        axes[1, 1].set_xlabel('è½®æ¬¡')
        axes[1, 1].set_ylabel('å­¦ä¹ ç‡')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].set_yscale('log')
        
        plt.suptitle(f'è®­ç»ƒè¿›åº¦ - ç¬¬ {epoch+1} è½®', fontsize=16)
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        viz_path = os.path.join(save_dir, f'training_progress_epoch_{epoch+1}.png')
        plt.savefig(viz_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    def create_confusion_matrix(self, val_loader, save_dir, epoch):
        """
        åˆ›å»ºæ··æ·†çŸ©é˜µï¼ˆå‚è€ƒCoastSatçš„è¯„ä¼°æ–¹æ³•ï¼‰
        
        å‚æ•°:
        val_loader: DataLoader, éªŒè¯æ•°æ®åŠ è½½å™¨
        save_dir: str, ä¿å­˜ç›®å½•
        epoch: int, å½“å‰è½®æ¬¡
        """
        # ç¡®ä¿ä¸­æ–‡å­—ä½“æ­£ç¡®æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Liberation Sans', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        self.model.eval()
        all_preds = []
        all_true = []
        
        with torch.no_grad():
            for images, masks in val_loader:
                images, masks = images.to(self.device), masks.to(self.device)
                outputs = self.model(images)
                pred_masks = torch.argmax(outputs, dim=1)
                
                all_preds.extend(pred_masks.cpu().numpy().flatten())
                all_true.extend(masks.cpu().numpy().flatten())
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_true, all_preds)
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        fig, ax = plt.subplots(figsize=(8, 6))
        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
        ax.figure.colorbar(im, ax=ax)
        
        classes = ['å…¶ä»–', 'æµ·æ°´']
        ax.set(xticks=np.arange(cm.shape[1]),
               yticks=np.arange(cm.shape[0]),
               xticklabels=classes, yticklabels=classes,
               title=f'æ··æ·†çŸ©é˜µ - ç¬¬ {epoch+1} è½®',
               ylabel='çœŸå®æ ‡ç­¾',
               xlabel='é¢„æµ‹æ ‡ç­¾')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, format(cm[i, j], 'd'),
                       ha="center", va="center",
                       color="white" if cm[i, j] > thresh else "black")
        
        plt.tight_layout()
        cm_path = os.path.join(save_dir, f'confusion_matrix_epoch_{epoch+1}.png')
        plt.savefig(cm_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    def train(self, train_loader, val_loader, epochs=200, save_dir="./models"):
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆé›†æˆCoastSatçš„è®­ç»ƒç­–ç•¥ï¼‰
        
        å‚æ•°:
        train_loader: DataLoader, è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: DataLoader, éªŒè¯æ•°æ®åŠ è½½å™¨
        epochs: int, è®­ç»ƒè½®æ•°
        save_dir: str, æ¨¡å‹ä¿å­˜ç›®å½•
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        start_time = datetime.now()
        
        best_val_loss = float('inf')
        best_iou = 0.0
        patience_counter = 0
        max_patience = 20  # æ—©åœè€å¿ƒå€¼
        
        print(f"\n=== å¼€å§‹è®­ç»ƒæµ·æ°´åˆ†å‰²æ¨¡å‹ ===")
        print(f"è®­ç»ƒè®¾å¤‡: {self.device}")
        print(f"è®­ç»ƒè½®æ•°: {epochs}")
        print(f"æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        print(f"ä¿å­˜ç›®å½•: {save_dir}\n")
        
        for epoch in range(epochs):
            epoch_start_time = datetime.now()
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_batches = 0
            
            print(f"è½®æ¬¡ {epoch+1}/{epochs}")
            print("-" * 50)
            
            for batch_idx, (images, masks) in enumerate(train_loader):
                images, masks = images.to(self.device), masks.to(self.device)
                
                self.optimizer.zero_grad()
                outputs = self.model(images)
                loss = self.criterion(outputs, masks)
                loss.backward()
                self.optimizer.step()
                
                train_loss += loss.item()
                train_batches += 1
                
                # æ¯10ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡è¿›åº¦
                if batch_idx % 10 == 0:
                    print(f'  æ‰¹æ¬¡ {batch_idx+1}/{len(train_loader)}, æŸå¤±: {loss.item():.4f}')
            
            train_loss /= train_batches
            
            # éªŒè¯é˜¶æ®µ
            val_metrics = self.validate_model(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_metrics['loss'])
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history['train_losses'].append(train_loss)
            self.training_history['val_losses'].append(val_metrics['loss'])
            self.training_history['accuracies'].append(val_metrics['accuracy'])
            self.training_history['iou_scores'].append(val_metrics['iou'])
            self.training_history['learning_rates'].append(current_lr)
            
            # è®¡ç®—è½®æ¬¡è€—æ—¶
            epoch_time = (datetime.now() - epoch_start_time).total_seconds()
            
            # æ‰“å°è½®æ¬¡ç»“æœ
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_metrics['loss']:.4f}")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {val_metrics['accuracy']:.4f}")
            print(f"  IoUåˆ†æ•°: {val_metrics['iou']:.4f}")
            print(f"  å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"  è½®æ¬¡è€—æ—¶: {epoch_time:.1f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºIoUåˆ†æ•°ï¼‰
            if val_metrics['iou'] > best_iou:
                best_iou = val_metrics['iou']
                best_val_loss = val_metrics['loss']
                self.training_history['best_model_epoch'] = epoch
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                model_path = os.path.join(save_dir, 'best_water_segmentation_model.pth')
                torch.save(self.model.state_dict(), model_path)
                print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (IoU: {best_iou:.4f})")
            else:
                patience_counter += 1
                print(f"  ç­‰å¾…æ”¹è¿›... ({patience_counter}/{max_patience})")
            
            # åˆ›å»ºè®­ç»ƒå¯è§†åŒ–
            if (epoch + 1) % 5 == 0 or epoch == 0:  # æ¯5è½®æˆ–ç¬¬ä¸€è½®åˆ›å»ºå¯è§†åŒ–
                self.create_training_visualization(epoch, train_loss, val_metrics, save_dir)
                self.create_confusion_matrix(val_loader, save_dir, epoch)
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= max_patience:
                print(f"\næ—©åœè§¦å‘ï¼å·²è¿ç»­ {max_patience} è½®æ— æ”¹è¿›")
                break
            
            print(f"  å½“å‰æœ€ä½³IoU: {best_iou:.4f} (ç¬¬ {self.training_history['best_model_epoch']+1} è½®)\n")
        
        # è®­ç»ƒå®Œæˆ
        total_time = (datetime.now() - start_time).total_seconds()
        self.training_history['training_time'] = total_time
        
        print("=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æ€»è€—æ—¶: {total_time//60:.0f}åˆ† {total_time%60:.0f}ç§’")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"æœ€ä½³IoUåˆ†æ•°: {best_iou:.4f}")
        print(f"æœ€ä½³æ¨¡å‹è½®æ¬¡: {self.training_history['best_model_epoch']+1}")
        print(f"æ¨¡å‹ä¿å­˜ä½ç½®: {os.path.join(save_dir, 'best_water_segmentation_model.pth')}")
        
        # ä¿å­˜å®Œæ•´è®­ç»ƒå†å²
        history_path = os.path.join(save_dir, 'training_history.pkl')
        with open(history_path, 'wb') as f:
            pickle.dump(self.training_history, f)
        
        # åˆ›å»ºæœ€ç»ˆè®­ç»ƒæŠ¥å‘Š
        self.create_final_training_report(save_dir)
        
        print("=" * 60)
        
        return self.training_history
    
    def create_final_training_report(self, save_dir):
        """
        åˆ›å»ºæœ€ç»ˆè®­ç»ƒæŠ¥å‘Šï¼ˆå‚è€ƒCoastSatçš„æŠ¥å‘Šé£æ ¼ï¼‰
        
        å‚æ•°:
        save_dir: str, ä¿å­˜ç›®å½•
        """
        # ç¡®ä¿ä¸­æ–‡å­—ä½“æ­£ç¡®æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Liberation Sans', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        
        # æŸå¤±å¯¹æ¯”å›¾
        axes[0, 0].plot(self.training_history['train_losses'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        axes[0, 0].plot(self.training_history['val_losses'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        axes[0, 0].axvline(x=self.training_history['best_model_epoch'], color='g', 
                          linestyle='--', alpha=0.7, label='æœ€ä½³æ¨¡å‹')
        axes[0, 0].set_title('æŸå¤±å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[0, 0].set_ylabel('æŸå¤±å€¼')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(self.training_history['accuracies'], 'g-', linewidth=2)
        axes[0, 1].axvline(x=self.training_history['best_model_epoch'], color='r', 
                          linestyle='--', alpha=0.7, label='æœ€ä½³æ¨¡å‹')
        axes[0, 1].set_title('éªŒè¯å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[0, 1].set_ylabel('å‡†ç¡®ç‡')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # IoUåˆ†æ•°æ›²çº¿
        axes[0, 2].plot(self.training_history['iou_scores'], 'm-', linewidth=2)
        axes[0, 2].axvline(x=self.training_history['best_model_epoch'], color='r', 
                          linestyle='--', alpha=0.7, label='æœ€ä½³æ¨¡å‹')
        axes[0, 2].set_title('IoUåˆ†æ•°å˜åŒ–', fontsize=14, fontweight='bold')
        axes[0, 2].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[0, 2].set_ylabel('IoUåˆ†æ•°')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡å˜åŒ–
        axes[1, 0].plot(self.training_history['learning_rates'], 'orange', linewidth=2)
        axes[1, 0].set_title('å­¦ä¹ ç‡è°ƒæ•´', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('è®­ç»ƒè½®æ¬¡')
        axes[1, 0].set_ylabel('å­¦ä¹ ç‡')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
        
        # è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        axes[1, 1].axis('off')
        stats_text = f"""
        è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        
        æ€»è®­ç»ƒè½®æ¬¡: {len(self.training_history['train_losses'])}
        æœ€ä½³æ¨¡å‹è½®æ¬¡: {self.training_history['best_model_epoch'] + 1}
        æœ€ä½³éªŒè¯æŸå¤±: {min(self.training_history['val_losses']):.4f}
        æœ€ä½³IoUåˆ†æ•°: {max(self.training_history['iou_scores']):.4f}
        æœ€ä½³å‡†ç¡®ç‡: {max(self.training_history['accuracies']):.4f}
        è®­ç»ƒæ€»è€—æ—¶: {self.training_history['training_time']//60:.0f}åˆ†{self.training_history['training_time']%60:.0f}ç§’
        """
        axes[1, 1].text(0.1, 0.9, stats_text, fontsize=12, verticalalignment='top',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
        
        # æ€§èƒ½åˆ†å¸ƒ
        final_metrics = [
            self.training_history['val_losses'][-1],
            self.training_history['accuracies'][-1],
            self.training_history['iou_scores'][-1]
        ]
        metric_names = ['éªŒè¯æŸå¤±', 'å‡†ç¡®ç‡', 'IoUåˆ†æ•°']
        colors = ['red', 'green', 'magenta']
        
        axes[1, 2].bar(metric_names, final_metrics, color=colors, alpha=0.7)
        axes[1, 2].set_title('æœ€ç»ˆæ¨¡å‹æ€§èƒ½', fontsize=14, fontweight='bold')
        axes[1, 2].set_ylabel('åˆ†æ•°')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(final_metrics):
            axes[1, 2].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')
        
        plt.suptitle('ğŸŒŠ æµ·æ°´åˆ†å‰²æ¨¡å‹è®­ç»ƒæŠ¥å‘Š', fontsize=18, fontweight='bold', y=0.98)
        plt.tight_layout()
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(save_dir, 'final_training_report.png')
        plt.savefig(report_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def prepare_dataset(self, images_dir, labels_dir):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®é›†
        
        å‚æ•°:
        images_dir: str, å›¾åƒç›®å½•
        labels_dir: str, æ ‡æ³¨ç›®å½•
        
        è¿”å›:
        tuple: (train_loader, val_loader)
        """
        # è·å–å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶å¯¹
        image_files = []
        label_files = []
        
        for img_file in os.listdir(images_dir):
            if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):
                img_path = os.path.join(images_dir, img_file)
                
                # æŸ¥æ‰¾å¯¹åº”çš„æ ‡æ³¨æ–‡ä»¶
                base_name = os.path.splitext(img_file)[0]
                label_path = os.path.join(labels_dir, f"{base_name}.json")
                
                if os.path.exists(label_path):
                    image_files.append(img_path)
                    label_files.append(label_path)
        
        print(f"æ‰¾åˆ° {len(image_files)} å¯¹å›¾åƒ-æ ‡æ³¨æ–‡ä»¶")
        
        if len(image_files) == 0:
            raise ValueError("æœªæ‰¾åˆ°åŒ¹é…çš„å›¾åƒ-æ ‡æ³¨æ–‡ä»¶å¯¹")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        print("\n=== æ•°æ®è´¨é‡æ£€æŸ¥ ===")
        valid_pairs = []
        for img_path, label_path in zip(image_files, label_files):
            try:
                # æ£€æŸ¥å›¾åƒ
                img = Image.open(img_path)
                if img.size[0] < 50 or img.size[1] < 50:
                    print(f"âš ï¸  è·³è¿‡å°ºå¯¸è¿‡å°çš„å›¾åƒ: {os.path.basename(img_path)}")
                    continue
                
                # æ£€æŸ¥æ ‡æ³¨
                with open(label_path, 'r', encoding='utf-8') as f:
                    label_data = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æµ·æ°´æ ‡æ³¨
                has_water = False
                for shape in label_data.get('shapes', []):
                    if shape['label'].lower() in ['water', 'sea', 'æµ·æ°´', 'æ°´ä½“']:
                        has_water = True
                        break
                
                if has_water:
                    valid_pairs.append((img_path, label_path))
                else:
                    print(f"âš ï¸  è·³è¿‡æ— æµ·æ°´æ ‡æ³¨çš„æ–‡ä»¶: {os.path.basename(label_path)}")
                    
            except Exception as e:
                print(f"âš ï¸  è·³è¿‡æŸåçš„æ–‡ä»¶å¯¹: {os.path.basename(img_path)} - {e}")
        
        if len(valid_pairs) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒ-æ ‡æ³¨æ–‡ä»¶å¯¹")
        
        image_files, label_files = zip(*valid_pairs)
        print(f"âœ“ æœ‰æ•ˆæ–‡ä»¶å¯¹: {len(image_files)}")
        
        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        train_imgs, val_imgs, train_labels, val_labels = train_test_split(
            image_files, label_files, test_size=0.2, random_state=42, shuffle=True
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = WaterSegmentationDataset(
            train_imgs, train_labels, transform=self.train_transform
        )
        val_dataset = WaterSegmentationDataset(
            val_imgs, val_labels, transform=self.val_transform
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)
        
        print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        print("=" * 40)
        
        return train_loader, val_loader

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== æµ·æ°´åŒºåŸŸè¯­ä¹‰åˆ†å‰²æ¨¡å‹è®­ç»ƒç¨‹åº ===")
    print("åŸºäºlabelmeæ ‡æ³¨çš„æµ·æ°´åŒºåŸŸè®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹")
    print("\nğŸ“‹ æ¨èå·¥ä½œæµç¨‹:")
    print("1. ä½¿ç”¨ tif_to_image.py å°†TIFæ–‡ä»¶è½¬æ¢ä¸ºPNG (./labelme_images/converted/)")
    print("2. ä½¿ç”¨ labelme å¯¹PNGå›¾åƒè¿›è¡Œæ ‡æ³¨ (ä¿å­˜åˆ° ./labelme_images/annotations/)")
    print("3. è¿è¡Œæœ¬ç¨‹åºè¿›è¡Œæ¨¡å‹è®­ç»ƒ")
    print("4. ä½¿ç”¨ predict_coastline.py è¿›è¡Œæµ·å²¸çº¿é¢„æµ‹")
    
    # è®­ç»ƒæ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    trainer = WaterSegmentationTrainer(device=device)
    
    images_dir = input("è¯·è¾“å…¥å›¾åƒç›®å½•è·¯å¾„ (é»˜è®¤: ./labelme_images/converted): ").strip()
    if not images_dir:
        images_dir = "./labelme_images/converted"
    
    labels_dir = input("è¯·è¾“å…¥æ ‡æ³¨ç›®å½•è·¯å¾„ (é»˜è®¤: ./labelme_images/annotations/): ").strip()
    if not labels_dir:
        labels_dir = "./labelme_images/annotations/"
    
    try:
        train_loader, val_loader = trainer.prepare_dataset(images_dir, labels_dir)
        print("\n=== å¼€å§‹è®­ç»ƒæ¨¡å‹ ===")
        print("è®­ç»ƒå‚æ•°:")
        print(f"- å›¾åƒç›®å½•: {images_dir}")
        print(f"- æ ‡æ³¨ç›®å½•: {labels_dir}")
        print(f"- è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
        print(f"- éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
        print(f"- è®¡ç®—è®¾å¤‡: {device}")
        
        epochs = int(input("è¯·è¾“å…¥è®­ç»ƒè½®æ•° (é»˜è®¤: 200): ") or "200")
        save_dir = input("è¯·è¾“å…¥æ¨¡å‹ä¿å­˜ç›®å½• (é»˜è®¤: ./models): ").strip() or "./models"
        
        trainer.train(train_loader, val_loader, epochs=epochs, save_dir=save_dir)
        
    except Exception as e:
        print(f"è®­ç»ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
